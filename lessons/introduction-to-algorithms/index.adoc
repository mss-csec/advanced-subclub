---
liquid: true
layout: lesson
category: advanced
lesson: 5
---
= Introduction to Algorithms

algorithm:: _n._ a process or set of rules to be followed in calculations or other problem-solving operations

'''

When we make a program, all we are doing is writing a set of instructions for computer to follow.
These instructions, of course, must be perfect -- they have to be simple enough for a computer to follow, since computers only know so much and can't interpret a programmer's intent.

With the introduction of modular programming, many oft-repeated calculations could be encapsulated into a function, and later these functions into libraries.
Standard yet complicated procedures -- like getting an integral inputfootnote:[``scanf()`` or ``std::cin``] -- could be implemented with a simple function call.

A simple program which takes two integral values and spits out its sum could be simplified to two variable decalarations, and two lines of code.
It's juat a simple *algorithm*.

Most algorithms, however, aren't so simple. In Computer Science, algorithms are about somehow manipulating memory to solve computational problems in a timely manner -- computers, after all, only know how to manipulate memory.

== Time Complexity: Measuring an Algorithm

It certainly wouldn't be a stretch to say that some algorithms are better than others.
If, for example, you had the following problem:
[quote, problem statement]
____
Given a sorted (least-to-greatest) array of size ``n``, find the index of a value ``m``.
If it does not exist, result ``-1``.
____

One way would be to iterate along the list, checking each and every element to see if it's the desired value.
This is called the *linear search*.

Another way, commonly called the *binary search*, leverages the fact that the array is sorted.
If one starts from the middle, they can easily cut the problem to half its size, as the desired value is on one side or the other and figuring out which side it's on relies on a simple comparison.

image::https://www.codingame.com/servlet/fileservlet?id=7769264647488[Visualization of binary and linear search]

Clearly, both will work -- both algorithms will eventually find the answer.
But you can also make the observation that the binary search is _better_.
It's not because it's more sophisticated, but rather that it's faster.
If you had a 1024-element array, you can expect the following number of iterations for each algorithm:
[cols="1,2,2",options="header"]
|=======
| |Linear Search |Binary Search
|Best Case |1 iteration (once in 1024) |1 iteration (once in 1024)
|Average Case |512 iterations | 9 iterations
|Worst Case |1024 iterations (once in 1024) |10 iterations (once in 2)
|=======

So, is binary search fifty times faster than linear search?
Well, not exactly.

First of all, binary search requires more time per iteration, as it does at least one addition comparison. Secondly, binary search is less efficient over the cachefootnote:[linear search only requires switching out the cache every few (eight or so) iterations; binary search needs to switch it out almost every iteration].

But most importantly, binary search uses an entirely different approach from linear search, and it grows differently.
If you double the size of your input, linear search will take twice as many iterations on average and in the worst case.
Binary search, on the other hand, will take _one_ more iteration.

They scale differently, and when it comes to writing good algorithms, that's what matters.

Efficient scaling _really_ matters.
[quote, "CCC15S4: Convex Hull, input specifications", (emphasis added)]
____
The first line of input contains three integers K, N and M (1 &#8804; K &#8804; 200; 2 &#8804; N &#8804; *2000*; 1 &#8804; M &#8804; *10000*), each separated by one space.

The next M lines each contain 4 integers a~i~ b~i~ t~i~ h~i~ (1 &#8804; a~i~, b~i~ &#8804; N; 1 &#8804; t~i~ &#8804; *10^5^*; 0 &#8804; h~i~ &#8804; 200), each separated by one space. The i-th line in this set of M lines describes the i-th sea route (which runs from island a~i~ to island b~i~, takes t~i~ minutes and wears down the ship's hull by h~i~ centimetres). Notice that a~i~ &#8800; b~i~ (that is, the ends of a sea route are distinct islands).
____

=== Asymptotic Analysis

How do we score an algorithm quantitatively?
Obviously, we don't want to express the "time" an algorithm would take in comparison to other algorithms, because those numbers are useless.

Instead, we should try to show how it scales.
To do that, we use a technique called *asymptotic analysis*.
Formally, we do that by looking at what type of mathematical function best represents the running time of an algorithm.

Linear search takes linear time, so it scales linearly.
If you were to express this as a mathematical function, where \(T\) is time, \(n\) is the input size, and \(c\) is some proportionality coefficient, you'd have \(T=cn\).

With binary search, you may notice that it scales link:++https://en.wikipedia.org/wiki/Logarithm++[_logarithmically_].
For binary search, you'd express that with \(T=c\log{n}\).

There's a few problems with this approach, though.
That coefficient \(c\) varies from environment to environment, so it's not a very useful measure.
And the time isn't _always_ proportional to some function on the input size, as most algorithms for most problems can end earlier than the worst- or average case.
We just need to focus on _scaling_.

To deal with this, we developed link:++https://en.wikipedia.org/wiki/Big_O_notation++[*big-O notation*].

Formally,
[quote, Wikipedia (paraphrased), the free encyclopedia]
____
A function \(f(x)\) is \(O(g(x))\),

that is, \(f(x)=O(g(x))\text{ as }x\rightarrow\infty\) if and only if

\(pass:[|f(x)|<|Mg(x)|]\) for some constant \(M\) and for all \(x\) where \(x \gg 0\).
____

Essentially, what this means is that \(g(x)\) is the _upper bound_ of the _growth_ \(f(x)\).

Using this notation, we can say that linear search is \(O(n)\) and binary search \(O(\log{n})\) for \(n\), the size of our input array.

This might seem rather alien and unusual.
In the next section, while we discuss how we can develop algorithms, we'll also do some more work with time complexity.

== Developing Algorithms

Now that you're acquainted with terms and with notation, let's discuss solving a common algorithmfootnote:[this section explains many common sorting algorithms; to see detailed visualizations and analysis, go link:++https://www.toptal.com/developers/sorting-algorithms++[here].].

[quote, Problem Statement]
____
Given an unsorted array stem:[n] elements and an operator stem:[\lt] which orders these elements, sort the array from least to greatest. 
____

image:++https://upload.wikimedia.org/wikipedia/commons/f/f6/Selection_Sort_Animation.gif++[Selection Sort,200,200,role="right"] The brute-force approach is rather obvious: take the minimum element from the unsorted array, and push that onto the end of the sorted array you're constructing.
This is easily achieved in-placefootnote:[this means that you're not using any extra memory, and all the sorting occurs in the array itself; your memory usage is stem:[O(1)].] by searching one-by-one through the unsorted array, and then swapping that minimum element with the _first_ element of the unsorted array to construct the sorted array.

Our first sort is based on _selecting_ elements, so we'll call it link:++https://en.wikipedia.org/wiki/Selection_sort++[*selection sort*].

For each of the stem:[n] elements we have to add to construct the sorted array, we're making up to stem:[n-1] comparisons.
To be precise, we make exactly stem:[\frac{1}{2}n(n+1)] comparisons (and stem:[n-1] swaps).
As such, selection sort grows quadratically, and its time complexity is stem:[O(n^2)].

Clearly, it'll be pretty slow for large inputs.
Is there a better way?

You may notice that one of the worst problems with selection sort is that it's not *adaptable*: it does blind searches and nothing else.
When the input array is already nearly sorted, it won't get any faster.
Instead of iteratively searching, we could try iteratively _moving_ elements closer to where they should be.

image:++https://upload.wikimedia.org/wikipedia/commons/2/2d/Bubble_Sort_Animation.gif++[Bubble Sort,200,200,role="right"] We'll still move along the array one-by-one, but this time for every comparison we'll swap if appropriate.
You'll notice that it's much faster than selection sort when the array's already nearly sorted.

This sort is based on _bubbling_ elements up, one at a time, so we'll call it link:++https://en.wikipedia.org/wiki/Bubble_sort++[*bubble sort*].
It turns out, though, that it's usually _worse_ than selection sortfootnote:[70% as fast in Java].

For every _pass_ through the array, one more element is guaranteed to be sorted, so we need to make stem:[n] passes through the array.
And in every pass, we make stem:[n] comparisons and stem:[O(n)] swapsfootnote:[that simply means _up to_ stem:[n]; it's just big-O notation]. That makes for an overall time complexity of... stem:[O(n^2)]footnote:[it's still slower than selection sort because time complexity _ignores_ the "hidden constant multiple": it grows in the same manner, but bubble sort still takes more overhead].

Let's take a look at selection sort again.
Selection sort's main bottleneck is finding the answer to "where do we take the next element out?", which requires a search among all unordered elements.
If we change that question to "where do we _put_ the next element _in_?", then we can leverage the fact that we'll be inserting into a _sorted_ list, and finding where it should go doesn't require a search among _all_ unordered elements.
We can just insert the next element of the unsorted array.

image:++https://upload.wikimedia.org/wikipedia/commons/a/ad/Insertion_Sort_Animation.gif++[Insertion Sort, 200,200,role="right"]
Before we go crazy implementing a binary search to figure out the insertion site, consider that to insert an element requires shifting everything after it once to the right.
We may was well just add the element to the end, and _bubble it down_ to the right position.

Since we came up with this sort from the idea of _inserting_, we'll call this link:++https://en.wikipedia.org/wiki/Insertion_sort++[*insertion sort*].
This one really is faster than selection sort.

For each of the stem:[n] elements we have to add to the sorted array, we make stem:[O(n)] comparisons and swaps for an overall time complexity of, once again, stem:[O(n^2)].
But this time, it really is faster.

If we know the largest size an element can be away from its proper position, there's another way to write the time complexity: for an input of size stem:[n] where the maximal displacement is stem:[k], for each stem:[n] elements to add, we make stem:[O(k)] comparisons and swaps for an overall time complexity of stem:[O(nk)].
Since stem:[k\leq n], it's sure to be faster than just stem:[O(n^2)].




